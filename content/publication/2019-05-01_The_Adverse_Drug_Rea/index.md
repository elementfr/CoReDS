+++
title = "The Adverse Drug Reactions From Patient Reports in Social Media Project: Protocol for an Evaluation Against a Gold Standard."
date = "2019-05-01"
authors = ["Arnoux-Guenegou Armelle", "Girardeau Yannick", "Chen Xiaoyi", "Deldossi Myrtille", "Aboukhamis Rim", "Faviez Carole", "Dahamna Badisse", "Karapetiantz Pierre", "Guillemin-Lanne Sylvie", "Lillo-Le Louet Agnes", "Texier Nathalie", "Burgun Anita", "Katsahian Sandrine"]
publication_types = ["2"]
publication = "JMIR research protocols, https://doi.org/10.2196/11448"
publication_short = "JMIR research protocols, https://doi.org/10.2196/11448"
abstract = "BACKGROUND:Social media is a potential source of information on postmarketing drug safety surveillance that still remains unexploited nowadays. Information technology solutions aiming at extracting adverse reactions (ADRs) from posts on health forums require a rigorous evaluation methodology if their results are to be used to make decisions. First, a gold standard, consisting of manual annotations of the ADR by human experts from the corpus extracted from social media, must be implemented and its quality must be assessed. Second, as for clinical research protocols, the sample size must rely on statistical arguments. Finally, the extraction methods must target the relation between the drug and the disease (which might be either treated or caused by the drug) rather than simple co-occurrences in the posts.OBJECTIVE:We propose a standardized protocol for the evaluation of a software extracting ADRs from the messages on health forums. The study is conducted as part of the Adverse Drug Reactions from Patient Reports in Social Media project.METHODS:Messages from French health forums were extracted. Entity recognition was based on Racine Pharma lexicon for drugs and Medical Dictionary for Regulatory Activities terminology for potential adverse events (AEs). Natural language processing-based techniques automated the ADR information extraction (relation between the drug and AE entities). The corpus of evaluation was a random sample of the messages containing drugs and/or AE concepts corresponding to recent pharmacovigilance alerts. A total of 2 persons experienced in medical terminology manually annotated the corpus, thus creating the gold standard, according to an annotator guideline. We will evaluate our tool against the gold standard with recall, precision, and f-measure. Interannotator agreement, reflecting gold standard quality, will be evaluated with hierarchical kappa. Granularities in the terminologies will be further explored.RESULTS:Necessary and sufficient sample size was calculated to ensure statistical confidence in the assessed results. As we expected a global recall of 0.5, we needed at least 384 identified ADR concepts to obtain a 95% CI with a total width of 0.10 around 0.5. The automated ADR information extraction in the corpus for evaluation is already finished. The 2 annotators already completed the annotation process. The analysis of the performance of the ADR information extraction module as compared with gold standard is ongoing.CONCLUSIONS:This protocol is based on the standardized statistical methods from clinical research to create the corpus, thus ensuring the necessary statistical power of the assessed results. Such evaluation methodology is required to make the ADR information extraction software useful for postmarketing drug safety surveillance.INTERNATIONAL REGISTERED REPORT IDENTIFIER (IRRID) NlmCategory=UNASSIGNED:RR1-10.2196/11448. "
abstract_short = ""
image_preview = ""
selected = false
projects = []
tags = []
url_pdf = ""
url_preprint = ""
url_code = ""
url_dataset = ""
url_project = ""
url_slides = ""
url_video = ""
url_poster = ""
url_source = ""
math = true
highlight = true
[header]
image = ""
caption = ""
+++
<a href="https://www.scimagojr.com/journalsearch.php?q=21100967335&amp;tip=sid&amp;exact=no" title="SCImago Journal &amp; Country Rank"><img border="0" src="https://www.scimagojr.com/journal_img.php?id=21100967335" alt="SCImago Journal &amp; Country Rank"  /></a>
**SCImago Journal Rank (SJR)** est un indicateur de notoriété des revues indexées à partir de 1996 dans la base de données Scopus de l’éditeur Elsevier. Le SJR a été créé par le groupe de travail SCImago Research Group (SRG) de l’Université de Grenade et Alcana de Henares en Espagne.  
  
Le SJR d’une revue est le nombre de fois où un article de cette revue est cité par d’autres articles pendant les 3 ans qui suivent sa publication, chaque citation reçue étant pondérée par la notoriété de la revue citante. Les articles « citants » sont issus d’autres revues et de la revue notée. Les citations d’articles de la revue par des articles de cette même revue (on parle d’autocitations) sont ainsi incluses dans le calcul du SJR, mais dans une limite de 35 %. Dans le calcul du SJR, le nombre de citations reçues par une revue est rapporté au nombre d’articles publiés par la revue au cours des 3 années qui précèdent.  
  
L'ensemble des revues a été classé en fonction de leur SJR et divisé en quatre groupes égaux, quartiles. Q1 (vert) comprend le quart des journaux avec les valeurs les plus élevées, Q2 (jaune) les deuxièmes valeurs les plus élevées, Q3 (orange) les troisièmes valeurs les plus élevées et Q4 (rouge) les valeurs les plus faibles.  
  
Différent entre le **SJR** et l'**Impact Factor** :  
- Le SJR est calculé pour une période de citation de 3 ans. Il tient compte de la notoriété des revues citantes. Il inclut de façon limitée les autocitations d’une revue ;  
- L'Impact Factor est calculé pour une période de citation de 2 ans. Il ne tient pas compte de la notoriété des revues citantes. Il inclut toutes les autocitations d’une revue.
